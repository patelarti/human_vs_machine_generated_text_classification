{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# BERT for classification"
      ],
      "metadata": {
        "id": "W073gqDlu_0f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## python libraries"
      ],
      "metadata": {
        "id": "C78TA9I1wvVo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dmmK6MCMtyZL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "import time\n",
        "import datetime\n",
        "import gc\n",
        "import random\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler,random_split\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, f1_score, confusion_matrix\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# from sklearn.metrics import f1_score, confusion_matrix\n",
        "\n",
        "import transformers\n",
        "from transformers import BertForSequenceClassification, AdamW, BertConfig,BertTokenizer,get_linear_schedule_with_warmup\n",
        "\n",
        "nltk.download('stopwords')\n",
        "sw = stopwords.words('english')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device('cuda' if use_cuda else 'cpu')\n",
        "if use_cuda:\n",
        "    torch.cuda.manual_seed(0)\n",
        "print(device)\n",
        "print(\"Using GPU: {}\".format(use_cuda))"
      ],
      "metadata": {
        "id": "ei5a1sVtuasG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset loading"
      ],
      "metadata": {
        "id": "x9WWDamjvU8Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade gdown\n",
        "!gdown --folder https://drive.google.com/drive/folders/1CAbb3DjrOPBNm0ozVBfhvrEh9P9rAppc\n",
        "!rm -rf /content/SubtaskA/subtaskA_dev_multilingual.jsonl\n",
        "!rm -rf /content/SubtaskA/subtaskA_train_multilingual.jsonl"
      ],
      "metadata": {
        "id": "aL7ELnRmvMuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/SubtaskA/subtaskA_train_monolingual.jsonl') as f:\n",
        "    data = [json.loads(line) for line in f]\n",
        "df = pd.DataFrame(data)\n",
        "ax = sns.countplot(x=\"label\", data=df)"
      ],
      "metadata": {
        "id": "O1WWxCmAu5Sd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data preprocessing"
      ],
      "metadata": {
        "id": "00HNlf1Ovive"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "\n",
        "    text = text.lower()\n",
        "\n",
        "    text = re.sub(r\"[^a-zA-Z?.!,Â¿]+\", \" \", text) # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "\n",
        "    text = re.sub(r\"http\\S+\", \"\",text) #Removing URLs\n",
        "    #text = re.sub(r\"http\", \"\",text)\n",
        "\n",
        "    html=re.compile(r'<.*?>')\n",
        "\n",
        "    text = html.sub(r'',text) #Removing html tags\n",
        "\n",
        "    punctuations = '@#!?+&*[]-%.:/();$=><|{}^' + \"'`\" + '_'\n",
        "    for p in punctuations:\n",
        "        text = text.replace(p,'') #Removing punctuations\n",
        "\n",
        "    text = [word.lower() for word in text.split() if word.lower() not in sw]\n",
        "\n",
        "    text = \" \".join(text) #removing stopwords\n",
        "\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    text = emoji_pattern.sub(r'', text) #Removing emojis\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "1JXs3uzsvZ8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['text'] = df['text'].apply(lambda x: clean_text(x))"
      ],
      "metadata": {
        "id": "TvQXDE2c329M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = df.text.values\n",
        "labels = df.label.values"
      ],
      "metadata": {
        "id": "1vdqOQDo34P9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "BERT Tokenizer"
      ],
      "metadata": {
        "id": "9wtf2o8_4CyH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "metadata": {
        "id": "Xt69fWsx4EzJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(' Original: ', texts[0])\n",
        "\n",
        "# Print the sentence split into tokens.\n",
        "print('Tokenized: ', tokenizer.tokenize(texts[0]))\n",
        "\n",
        "# Print the sentence mapped to token ids.\n",
        "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(texts[0])))"
      ],
      "metadata": {
        "id": "J9HnXyno4Hlg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = 0\n",
        "\n",
        "# For each and every sentence...\n",
        "for sent in texts:\n",
        "\n",
        "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
        "    input_ids = tokenizer.encode(sent, padding=True, truncation=True, max_length=256, add_special_tokens = True)\n",
        "    # input_ids = tokenizer.encode(sent, padding=True, truncation=True, add_special_tokens=True)\n",
        "\n",
        "    # Update the maximum sentence length.\n",
        "    max_len = max(max_len, len(input_ids))\n",
        "\n",
        "print('Max sentence length: ', max_len)"
      ],
      "metadata": {
        "id": "4YpnNNYD4fS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "# For every text...\n",
        "for text in texts:\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        text,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = max_len,           # Pad & truncate all sentences.\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks.\n",
        "                        return_tensors = 'pt',\n",
        "                        truncation=True# Return pytorch tensors.\n",
        "                   )\n",
        "\n",
        "    # Add the encoded sentence to the list.\n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "\n",
        "    # And its attention mask (simply differentiates padding from non-padding).\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors.\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "# Print sentence 0, now as a list of IDs.\n",
        "print('Original: ', texts[0])\n",
        "print('Token IDs:', input_ids[0])"
      ],
      "metadata": {
        "id": "KuTzCqMo4lMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train-validation split\n",
        "\n",
        "80% of data is split into train and 20% to validation sets."
      ],
      "metadata": {
        "id": "Pl4WAGJ144id"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine the training inputs into a TensorDataset.\n",
        "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "# Create a 80-20 train-validation split.\n",
        "\n",
        "# Calculate the number of samples to include in each set.\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset)  - train_size\n",
        "\n",
        "# Divide the dataset by randomly selecting samples.\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "print('{:>5,} training samples'.format(train_size))\n",
        "print('{:>5,} testing samples'.format(val_size))\n",
        "val_original_labels = val_dataset[:][2].tolist()"
      ],
      "metadata": {
        "id": "daX9JZA24z_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print('testing labels:')\n",
        "temp = []\n",
        "\n",
        "temp = val_dataset[:][2].tolist()\n",
        "print(len(temp))"
      ],
      "metadata": {
        "id": "2RLViDMt_U2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "\n",
        "# Create the DataLoaders for our training and validation sets.\n",
        "# We'll take training samples in random order.\n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,  # The training samples.\n",
        "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
        "            batch_size = batch_size # Trains with this batch size.\n",
        "        )\n",
        "\n",
        "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
        "validation_dataloader = DataLoader(\n",
        "            val_dataset, # The validation samples.\n",
        "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
        "            batch_size = batch_size # Evaluate with this batch size.\n",
        "        )"
      ],
      "metadata": {
        "id": "eipy04-H5bMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top.\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
        "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")\n",
        "\n",
        "\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "LJabIRt354SY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # args.learning_rate - default is 5e-5,\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )"
      ],
      "metadata": {
        "id": "A77C_3706IPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine tuning the model"
      ],
      "metadata": {
        "id": "3trvAQEq9wav"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 1\n",
        "\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ],
      "metadata": {
        "id": "B4prSjw26TFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "metadata": {
        "id": "ZBp98Vmz-AnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "metadata": {
        "id": "S7KKvId8AiiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed_val = 42\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "training_stats = []\n",
        "predictions = []\n",
        "\n",
        "# Measure the total training time for the whole run.\n",
        "total_t0 = time.time()\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "\n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    # Perform one full pass over the training set.\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "    total_train_loss = 0\n",
        "    model.train()\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(b_input_ids,\n",
        "                             token_type_ids=None,\n",
        "                             attention_mask=b_input_mask,\n",
        "                             labels=b_labels)\n",
        "        loss = output.loss\n",
        "        total_train_loss += loss.item()\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
        "\n",
        "    # Measure how long this epoch took.\n",
        "    training_time = format_time(time.time() - t0)\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "    t0 = time.time()\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "    # Tracking variables\n",
        "    total_eval_accuracy = 0\n",
        "    best_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output= model(b_input_ids,\n",
        "                                   token_type_ids=None,\n",
        "                                   attention_mask=b_input_mask,\n",
        "                                   labels=b_labels)\n",
        "        loss = output.loss\n",
        "        total_eval_loss += loss.item()\n",
        "        # Move logits and labels to CPU if we are using GPU\n",
        "        logits = output.logits\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        pred_flat = np.argmax(logits, axis=1).flatten()\n",
        "\n",
        "        predictions.extend(list(pred_flat))\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "    # Report the final accuracy for this validation run.\n",
        "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "    # Measure how long the validation run took.\n",
        "    validation_time = format_time(time.time() - t0)\n",
        "    if avg_val_accuracy > best_eval_accuracy:\n",
        "        torch.save(model, 'bert_model')\n",
        "        best_eval_accuracy = avg_val_accuracy\n",
        "\n",
        "    # Record all statistics from this epoch.\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'Valid. Accur.': avg_val_accuracy,\n",
        "            'Training Time': training_time,\n",
        "            'Validation Time': validation_time\n",
        "        }\n",
        "    )\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
        "print(classification_report(val_original_labels,predictions))\n",
        "cnf_matrix = confusion_matrix(val_original_labels,predictions)\n",
        "group_names = ['TN','FP','FN','TP']\n",
        "group_counts = [\"{0:0.0f}\".format(value) for value in cnf_matrix.flatten()]\n",
        "labels = [f\"{v1}\\n{v2}\" for v1, v2 in zip(group_names,group_counts)]\n",
        "labels = np.asarray(labels).reshape(2,2)\n",
        "sns.heatmap(cnf_matrix, annot=labels, fmt='', cmap='Blues');"
      ],
      "metadata": {
        "id": "x-gNASlkBe6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the best model"
      ],
      "metadata": {
        "id": "b6WPjS3jBk7U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.load('bert_model')"
      ],
      "metadata": {
        "id": "hlMd_t5IBmcN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test on validation dataset"
      ],
      "metadata": {
        "id": "5No3A6ECeOVN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/SubtaskA/subtaskA_dev_monolingual.jsonl') as f:\n",
        "    data = [json.loads(line) for line in f]\n",
        "df_test = pd.DataFrame(data)\n",
        "\n",
        "\n",
        "df_test['text'] = df_test['text'].apply(lambda x: clean_text(x))\n",
        "test_texts = df_test.text.values\n",
        "labels = df_test.label.values"
      ],
      "metadata": {
        "id": "E1_XdmBfDpGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_input_ids = []\n",
        "test_attention_masks = []\n",
        "for test_text in test_texts:\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        test_text,\n",
        "                        add_special_tokens = True,\n",
        "                        max_length = max_len,\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,\n",
        "                        return_tensors = 'pt',\n",
        "                   )\n",
        "    test_input_ids.append(encoded_dict['input_ids'])\n",
        "    test_attention_masks.append(encoded_dict['attention_mask'])\n",
        "test_input_ids = torch.cat(test_input_ids, dim=0)\n",
        "test_attention_masks = torch.cat(test_attention_masks, dim=0)\n",
        "labels = torch.tensor(labels)"
      ],
      "metadata": {
        "id": "auy4naKpDxjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = TensorDataset(test_input_ids, test_attention_masks,labels)\n",
        "test_dataloader = DataLoader(\n",
        "            test_dataset, # The validation samples.\n",
        "            sampler = SequentialSampler(test_dataset), # Pull out batches sequentially.\n",
        "            batch_size = batch_size # Evaluate with this batch size.\n",
        "        )"
      ],
      "metadata": {
        "id": "kp-HkOYmD-72"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Running Validation...\")\n",
        "t0 = time.time()\n",
        "# Put the model in evaluation mode--the dropout layers behave differently\n",
        "# during evaluation.\n",
        "model.eval()\n",
        "# Tracking variables\n",
        "total_eval_accuracy = 0\n",
        "best_eval_accuracy = 0\n",
        "total_eval_loss = 0\n",
        "nb_eval_steps = 0\n",
        "\n",
        "predictions = []\n",
        "for batch in test_dataloader:\n",
        "    b_input_ids = batch[0].to(device)\n",
        "    b_input_mask = batch[1].to(device)\n",
        "    b_labels = batch[2].to(device)\n",
        "    # Tell pytorch not to bother with constructing the compute graph during\n",
        "    # the forward pass, since this is only needed for backprop (training).\n",
        "    with torch.no_grad():\n",
        "        output= model(b_input_ids,\n",
        "                                token_type_ids=None,\n",
        "                                attention_mask=b_input_mask,\n",
        "                                labels=b_labels)\n",
        "    loss = output.loss\n",
        "    total_eval_loss += loss.item()\n",
        "    # Move logits and labels to CPU if we are using GPU\n",
        "    logits = output.logits\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "    pred_flat = np.argmax(logits, axis=1).flatten()\n",
        "\n",
        "    predictions.extend(list(pred_flat))\n",
        "    # Calculate the accuracy for this batch of test sentences, and\n",
        "    # accumulate it over all batches.\n",
        "    total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "# Report the final accuracy for this validation run.\n",
        "avg_val_accuracy = total_eval_accuracy / len(test_dataloader)\n",
        "\n",
        "\n",
        "print(classification_report(df_test['label'],predictions))\n",
        "cnf_matrix = confusion_matrix(df_test['label'],predictions)\n",
        "group_names = ['TN','FP','FN','TP']\n",
        "group_counts = [\"{0:0.0f}\".format(value) for value in cnf_matrix.flatten()]\n",
        "labels = [f\"{v1}\\n{v2}\" for v1, v2 in zip(group_names,group_counts)]\n",
        "labels = np.asarray(labels).reshape(2,2)\n",
        "sns.heatmap(cnf_matrix, annot=labels, fmt='', cmap='Blues');"
      ],
      "metadata": {
        "id": "8ssX4BO5EsJP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}