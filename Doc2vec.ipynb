{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Doc2Vec**"
      ],
      "metadata": {
        "id": "v9_cQiSmWoRH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Necessary Libraries"
      ],
      "metadata": {
        "id": "_q4mSLJeTXax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "import gensim\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from gensim.parsing.preprocessing import remove_stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "id": "QNVkFA9Ed9PY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read Data\n",
        "\n",
        "Reads the provided dataset (read .jsonl files) from pandas library, after that displays the top data using df.head(), as shown below the dataset has 4 text,label,\tmodel,\tsource and\tid"
      ],
      "metadata": {
        "id": "jXxaXYnAXhSy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ODKV1uh-dYHp"
      },
      "outputs": [],
      "source": [
        "!pip install gensim\n",
        "!pip install --upgrade gdown\n",
        "!gdown --folder https://drive.google.com/drive/folders/1CAbb3DjrOPBNm0ozVBfhvrEh9P9rAppc\n",
        "!rm -rf /content/SubtaskA/subtaskA_dev_multilingual.jsonl\n",
        "!rm -rf /content/SubtaskA/subtaskA_train_multilingual.jsonl"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/SubtaskA/subtaskA_train_monolingual.jsonl') as f:\n",
        "    data = [json.loads(line) for line in f]\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "with open('/content/SubtaskA/subtaskA_dev_monolingual.jsonl') as f:\n",
        "    val_data = [json.loads(line) for line in f]\n",
        "val_df = pd.DataFrame(val_data)\n",
        "\n",
        "ax = sns.countplot(x=\"label\", data=df)"
      ],
      "metadata": {
        "id": "3loGL6bzXpag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "sc1BkvE_Xv03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "LrK3c0Q-XxC2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sw = stopwords.words('english')\n",
        "def clean_text(text):\n",
        "\n",
        "    text = text.lower()\n",
        "\n",
        "    text = re.sub(r\"[^a-zA-Z?.!,Â¿]+\", \" \", text) # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "\n",
        "    text = re.sub(r\"http\\S+\", \"\",text) #Removing URLs\n",
        "    #text = re.sub(r\"http\", \"\",text)\n",
        "\n",
        "    html=re.compile(r'<.*?>')\n",
        "\n",
        "    text = html.sub(r'',text) #Removing html tags\n",
        "\n",
        "    punctuations = '@#!?+&*[]-%.:/();$=><|{}^' + \"'`\" + '_'\n",
        "    for p in punctuations:\n",
        "        text = text.replace(p,'') #Removing punctuations\n",
        "\n",
        "    text = [word.lower() for word in text.split() if word.lower() not in sw]\n",
        "\n",
        "    text = \" \".join(text) #removing stopwords\n",
        "\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    text = emoji_pattern.sub(r'', text) #Removing emojis\n",
        "\n",
        "    return text\n",
        "\n",
        "# df['text_clean'] = df['text'].apply(lambda x: clean_text(x))\n",
        "# df.head()"
      ],
      "metadata": {
        "id": "dg5fn_DqTx-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training and validation dataset cleaning"
      ],
      "metadata": {
        "id": "5GZwEWQfTxYU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#training dataset cleaning\n",
        "base = df[['text', 'label']]\n",
        "base.text = base.text.map(clean_text)\n",
        "lista_base = list(base.text)\n",
        "tagged_trained_data = [TaggedDocument(words=word_tokenize(str(_d).lower()), tags=[str(i)]) for i, _d in enumerate(lista_base)]\n",
        "\n",
        "model = gensim.models.doc2vec.Doc2Vec(vector_size=100, epochs=10)\n",
        "model.build_vocab(tagged_trained_data)\n",
        "model.train(tagged_trained_data, total_examples=model.corpus_count, epochs=10)\n",
        "similar_doc = model.docvecs.most_similar('0')\n",
        "# print(similar_doc[0])\n",
        "\n",
        "matrix = [] # <- Matriz com as 100 coordenadas de cada palavra.\n",
        "for i in range(len(base.text)):\n",
        "  tokens_aux = nltk.word_tokenize(lista_base[i])\n",
        "  matrix.append(model.infer_vector(tokens_aux))\n",
        "base_matriz = pd.DataFrame(matrix)\n",
        "\n",
        "#validation dataset cleaning\n",
        "val_base = val_df[['text', 'label']]\n",
        "\n",
        "\n",
        "val_base.text = val_base.text.map(clean_text)\n",
        "lista_val_base = list(val_base.text)\n",
        "\n",
        "val_matrix = [] # <- Matriz com as 100 coordenadas de cada palavra.\n",
        "for i in range(len(val_base.text)):\n",
        "  tokens_aux = nltk.word_tokenize(lista_val_base[i])\n",
        "  val_matrix.append(model.infer_vector(tokens_aux))\n",
        "val_base_matriz = pd.DataFrame(val_matrix)\n",
        "\n",
        "\n",
        "# Mudando nome das colunas para dim_\n",
        "colunas = val_base_matriz.columns\n",
        "nome_colunas = ['dim'+str(colunas[i]) for i in range(100)]\n",
        "\n",
        "val_base_matriz.columns = nome_colunas\n",
        "val_base_matriz['classe'] = val_base.label\n",
        "\n",
        "# from sklearn.model_selection import KFold, cross_val_score, cross_validate, GridSearchCV, train_test_split\n",
        "# k_folds = KFold(n_splits = 5)\n",
        "val_X = val_base_matriz.iloc[:,0:100]\n",
        "val_y = val_base_matriz.iloc[:,-1]\n"
      ],
      "metadata": {
        "id": "8fsZDzkqeMVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_matriz.to_csv(r'matrix_100.csv')\n",
        "val_base_matriz.to_csv(r'val_matrix_100.csv')\n",
        "# Mudando nome das colunas para dim_\n",
        "colunas = base_matriz.columns\n",
        "nome_colunas = ['dim'+str(colunas[i]) for i in range(100)]\n",
        "\n",
        "base_matriz.columns = nome_colunas\n",
        "base_matriz['classe'] = base.label\n"
      ],
      "metadata": {
        "id": "MPZ5cV_lqAl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold, cross_val_score, cross_validate, GridSearchCV, train_test_split\n",
        "k_folds = KFold(n_splits = 5)\n",
        "X = base_matriz.iloc[:,0:100]\n",
        "y = base_matriz.iloc[:,-1]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "print(len(y_train))"
      ],
      "metadata": {
        "id": "kMRhijd1qS6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic Regression"
      ],
      "metadata": {
        "id": "DHIJDXFHRjxf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix,accuracy_score\n",
        "modelo_RL = LogisticRegression()\n",
        "scores = cross_val_score(modelo_RL, X_train, y_train, cv=5)\n",
        "print(scores.mean())"
      ],
      "metadata": {
        "id": "d4wwia9zrSr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelo_RL.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "52HyWEXjrawx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_RL = modelo_RL.predict(X_test)"
      ],
      "metadata": {
        "id": "VRRgx7RaR40T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "print(classification_report(y_test, y_pred_RL))\n",
        "print(accuracy_score(y_test, y_pred_RL))\n",
        "print(confusion_matrix(y_test, y_pred_RL))\n",
        "\n",
        "\n",
        "cnf_matrix = confusion_matrix(y_test,y_pred_RL)\n",
        "group_names = ['TN','FP','FN','TP']\n",
        "group_counts = [\"{0:0.0f}\".format(value) for value in cnf_matrix.flatten()]\n",
        "labels = [f\"{v1}\\n{v2}\" for v1, v2 in zip(group_names,group_counts)]\n",
        "labels = np.asarray(labels).reshape(2,2)\n",
        "sns.heatmap(cnf_matrix, annot=labels, fmt='', cmap='Blues');"
      ],
      "metadata": {
        "id": "M4kTf8aXrgbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Validation Logistic Regression"
      ],
      "metadata": {
        "id": "0m9O8W-TSFCU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_LR = modelo_RL.predict(val_X)\n",
        "print(classification_report(val_y, y_pred_LR))\n",
        "print(accuracy_score(val_y, y_pred_LR))\n",
        "print(confusion_matrix(val_y, y_pred_LR))\n",
        "\n",
        "cnf_matrix = confusion_matrix(val_y,y_pred_LR)\n",
        "group_names = ['TN','FP','FN','TP']\n",
        "group_counts = [\"{0:0.0f}\".format(value) for value in cnf_matrix.flatten()]\n",
        "labels = [f\"{v1}\\n{v2}\" for v1, v2 in zip(group_names,group_counts)]\n",
        "labels = np.asarray(labels).reshape(2,2)\n",
        "sns.heatmap(cnf_matrix, annot=labels, fmt='', cmap='Blues');\n"
      ],
      "metadata": {
        "id": "4Ar6iDtNK9CE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decision Tree"
      ],
      "metadata": {
        "id": "oou82QZWS5Xo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "modelo_DT = DecisionTreeClassifier(random_state=0)\n",
        "#modelo_2.fit(X_train, y_train)\n",
        "para_DT = {'criterion':['gini','entropy'],'max_depth':[4,5,6,7,8,9,10,11,12,15,20,30,40,50,70,90,120,150]}\n",
        "grid_DT = GridSearchCV(modelo_DT, para_DT, cv=5)\n",
        "grid_DT.fit(X_train, y_train)\n",
        "print(grid_DT.best_params_, grid_DT.best_score_)\n",
        "print(cross_val_score(modelo_DT, X_train, y_train, cv=5).mean())"
      ],
      "metadata": {
        "id": "5mA2nhNhtTVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelo_DT = DecisionTreeClassifier(criterion='gini', max_depth=5)\n",
        "modelo_DT.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "6V90jSwltcug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_DT = modelo_DT.predict(X_test)"
      ],
      "metadata": {
        "id": "gEIHIKEot1Az"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test, y_pred_DT))\n",
        "print(accuracy_score(y_test, y_pred_DT))\n",
        "print(confusion_matrix(y_test, y_pred_DT))\n",
        "\n",
        "cnf_matrix = confusion_matrix(y_test,y_pred_DT)\n",
        "group_names = ['TN','FP','FN','TP']\n",
        "group_counts = [\"{0:0.0f}\".format(value) for value in cnf_matrix.flatten()]\n",
        "labels = [f\"{v1}\\n{v2}\" for v1, v2 in zip(group_names,group_counts)]\n",
        "labels = np.asarray(labels).reshape(2,2)\n",
        "sns.heatmap(cnf_matrix, annot=labels, fmt='', cmap='Blues');"
      ],
      "metadata": {
        "id": "vyKd5uCUt4uy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Validation Decision Tree"
      ],
      "metadata": {
        "id": "aswUwdbFTpEx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "y_pred_RL = modelo_DT.predict(X)\n",
        "print(classification_report(y, y_pred_RL))\n",
        "print(accuracy_score(y, y_pred_RL))\n",
        "print(confusion_matrix(y, y_pred_RL))\n",
        "\n",
        "cnf_matrix = confusion_matrix(y,y_pred_RL)\n",
        "group_names = ['TN','FP','FN','TP']\n",
        "group_counts = [\"{0:0.0f}\".format(value) for value in cnf_matrix.flatten()]\n",
        "labels = [f\"{v1}\\n{v2}\" for v1, v2 in zip(group_names,group_counts)]\n",
        "labels = np.asarray(labels).reshape(2,2)\n",
        "sns.heatmap(cnf_matrix, annot=labels, fmt='', cmap='Blues');\n"
      ],
      "metadata": {
        "id": "ilugp5GBxwvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest Classifier"
      ],
      "metadata": {
        "id": "8rcItRPRYcBn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "modelo_RF = RandomForestClassifier()\n",
        "para_RF = param_grid = { 'n_estimators': [50, 100, 200, 400], 'criterion':['gini','entropy'],\n",
        "              'max_depth': [None, 5, 10]}\n",
        "grid_RF = GridSearchCV(estimator=modelo_RF,\n",
        "                           param_grid=para_RF,\n",
        "                           cv=5)\n",
        "grid_RF.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "jN2tNENHuhFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(grid_RF.best_params_, grid_RF.best_score_)\n",
        "print(cross_val_score(modelo_RF, X_train, y_train, cv=5).mean())"
      ],
      "metadata": {
        "id": "6DMAxjmVuhIZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modelo_RF = RandomForestClassifier(criterion='entropy', n_estimators=200, max_depth=10)\n",
        "modelo_RF.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "LNeoRLZ0uhK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_RF = modelo_RF.predict(X_test)"
      ],
      "metadata": {
        "id": "C_4JHsMuuhOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_test, y_pred_RF))\n",
        "print(accuracy_score(y_test, y_pred_RF))\n",
        "print(confusion_matrix(y_test, y_pred_RF))\n",
        "\n",
        "cnf_matrix = confusion_matrix(y_test, y_pred_RF)\n",
        "group_names = ['TN','FP','FN','TP']\n",
        "group_counts = [\"{0:0.0f}\".format(value) for value in cnf_matrix.flatten()]\n",
        "labels = [f\"{v1}\\n{v2}\" for v1, v2 in zip(group_names,group_counts)]\n",
        "labels = np.asarray(labels).reshape(2,2)\n",
        "sns.heatmap(cnf_matrix, annot=labels, fmt='', cmap='Blues');"
      ],
      "metadata": {
        "id": "6NpnyENvx9fp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Validation Random Forest Classifier"
      ],
      "metadata": {
        "id": "F2H1xsNrgkde"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_LR = modelo_RL.predict(val_X)\n",
        "print(classification_report(val_y, y_pred_LR))\n",
        "print(accuracy_score(val_y, y_pred_LR))\n",
        "print(confusion_matrix(val_y, y_pred_LR))\n",
        "\n",
        "cnf_matrix = confusion_matrix(val_y,y_pred_LR)\n",
        "group_names = ['TN','FP','FN','TP']\n",
        "group_counts = [\"{0:0.0f}\".format(value) for value in cnf_matrix.flatten()]\n",
        "labels = [f\"{v1}\\n{v2}\" for v1, v2 in zip(group_names,group_counts)]\n",
        "labels = np.asarray(labels).reshape(2,2)\n",
        "sns.heatmap(cnf_matrix, annot=labels, fmt='', cmap='Blues');"
      ],
      "metadata": {
        "id": "prlbV_uCgqNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## KNN classifier"
      ],
      "metadata": {
        "id": "TX5JYEhSg8Tf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas.core.common import random_state\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "modelo_Knn = KNeighborsClassifier()\n",
        "para_Knn = {'p':[1,2], 'n_neighbors':[3,5,7,9,13,21,51,61,71,81,91,101]}\n",
        "grid_Knn = GridSearchCV(estimator=modelo_Knn,\n",
        "                           param_grid=para_Knn,\n",
        "                           cv=5)\n",
        "grid_Knn.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "gjSKdIEPvB9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(grid_Knn.best_params_, grid_Knn.best_score_)\n",
        "modelo_Knn = KNeighborsClassifier(n_neighbors=61, p=2)\n",
        "modelo_Knn.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "1A-Socz1vKd8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(cross_val_score(modelo_Knn, X, y, cv=5).mean())\n",
        "y_pred_Knn = modelo_Knn.predict(X_test)"
      ],
      "metadata": {
        "id": "gmE7B6RLvS8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_test, y_pred_Knn))\n",
        "print(accuracy_score(y_test, y_pred_Knn))\n",
        "print(confusion_matrix(y_test, y_pred_Knn))\n",
        "\n",
        "cnf_matrix = confusion_matrix(y_test, y_pred_Knn)\n",
        "group_names = ['TN','FP','FN','TP']\n",
        "group_counts = [\"{0:0.0f}\".format(value) for value in cnf_matrix.flatten()]\n",
        "labels = [f\"{v1}\\n{v2}\" for v1, v2 in zip(group_names,group_counts)]\n",
        "labels = np.asarray(labels).reshape(2,2)\n",
        "sns.heatmap(cnf_matrix, annot=labels, fmt='', cmap='Blues');"
      ],
      "metadata": {
        "id": "tv7HcY3RyEXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Validation KNN"
      ],
      "metadata": {
        "id": "uFOaKQ0ek97Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val_y_pred_knn = modelo_RL.predict(val_X)\n",
        "print(classification_report(val_y, val_y_pred_knn))\n",
        "print(accuracy_score(val_y, val_y_pred_knn))\n",
        "print(confusion_matrix(val_y, val_y_pred_knn))\n",
        "\n",
        "cnf_matrix = confusion_matrix(val_y,val_y_pred_knn)\n",
        "group_names = ['TN','FP','FN','TP']\n",
        "group_counts = [\"{0:0.0f}\".format(value) for value in cnf_matrix.flatten()]\n",
        "labels = [f\"{v1}\\n{v2}\" for v1, v2 in zip(group_names,group_counts)]\n",
        "labels = np.asarray(labels).reshape(2,2)\n",
        "sns.heatmap(cnf_matrix, annot=labels, fmt='', cmap='Blues');"
      ],
      "metadata": {
        "id": "gf7eTlvyk9Qs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SVM"
      ],
      "metadata": {
        "id": "CAH_V2q0lDBJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "random_state = 0\n",
        "modelo_svm = SVC()\n",
        "parametros_svm = {'C': [0.1, 1, 10, 100, 1000],\n",
        "              'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
        "              'kernel': ['rbf']}"
      ],
      "metadata": {
        "id": "JUKIuY4WvlLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid = GridSearchCV(modelo_svm, parametros_svm, refit = True, verbose = 3)\n",
        "\n",
        "# fitting the model for grid search\n",
        "grid.fit(X, y)"
      ],
      "metadata": {
        "id": "g8R0StVpvnqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Best Hyperparameters: \", grid.best_params_)\n",
        "print(\"Best Score: \", grid.best_score_)\n",
        "modelo_svm = SVC(C=10, gamma=0.001, kernel='rbf')\n",
        "modelo_svm.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "jSN1yE8Xvp1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_svm = modelo_svm.predict(X_test)"
      ],
      "metadata": {
        "id": "E9pm5HnbvwOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "print(classification_report(y_test, y_pred_svm))\n",
        "print(accuracy_score(y_test, y_pred_svm))\n",
        "print(confusion_matrix(y_test, y_pred_svm))\n",
        "\n",
        "cnf_matrix = confusion_matrix(y_test, y_pred_svm)\n",
        "group_names = ['TN','FP','FN','TP']\n",
        "group_counts = [\"{0:0.0f}\".format(value) for value in cnf_matrix.flatten()]\n",
        "labels = [f\"{v1}\\n{v2}\" for v1, v2 in zip(group_names,group_counts)]\n",
        "labels = np.asarray(labels).reshape(2,2)\n",
        "sns.heatmap(cnf_matrix, annot=labels, fmt='', cmap='Blues');"
      ],
      "metadata": {
        "id": "oueBuDVtv0OV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Validation SVM"
      ],
      "metadata": {
        "id": "vvisveS9lH5X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val_y_pred_svm = modelo_RL.predict(val_X)\n",
        "print(classification_report(val_y, val_y_pred_svm))\n",
        "print(accuracy_score(val_y, val_y_pred_svm))\n",
        "print(confusion_matrix(val_y, val_y_pred_svm))\n",
        "\n",
        "cnf_matrix = confusion_matrix(val_y,val_y_pred_svm)\n",
        "group_names = ['TN','FP','FN','TP']\n",
        "group_counts = [\"{0:0.0f}\".format(value) for value in cnf_matrix.flatten()]\n",
        "labels = [f\"{v1}\\n{v2}\" for v1, v2 in zip(group_names,group_counts)]\n",
        "labels = np.asarray(labels).reshape(2,2)\n",
        "sns.heatmap(cnf_matrix, annot=labels, fmt='', cmap='Blues');"
      ],
      "metadata": {
        "id": "LCsQj9WhlIF6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}